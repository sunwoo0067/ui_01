#!/usr/bin/env python3
"""
ì˜¤ë„ˆí´ëœ ë°°ì¹˜ íƒ€ì… ì „ì²´ ìˆ˜ì§‘
allItems GraphQL ì¿¼ë¦¬ë¡œ ì „ì²´ ì¹´íƒˆë¡œê·¸ ìˆ˜ì§‘
"""

import asyncio
import json
import hashlib
from datetime import datetime
from loguru import logger

from src.services.ownerclan_data_collector import OwnerClanDataCollector
from src.services.database_service import DatabaseService


async def collect_ownerclan_full_catalog():
    """ì˜¤ë„ˆí´ëœ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘"""
    db = DatabaseService()
    collector = OwnerClanDataCollector()
    
    supplier_id = "e458e4e2-cb03-4fc2-bff1-b05aaffde00f"
    account_name = "test_account"
    
    logger.info("="*70)
    logger.info("ğŸ¢ ì˜¤ë„ˆí´ëœ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘")
    logger.info("   ë°©ì‹: allItems GraphQL ì¿¼ë¦¬ (ì „ì²´ ìƒí’ˆ)")
    logger.info("="*70)
    
    start_time = datetime.now()
    
    # allItems GraphQL ì¿¼ë¦¬ (ì „ì²´ ìƒí’ˆ)
    query = """
    query {
        allItems(first: 1000) {
            pageInfo {
                hasNextPage
                endCursor
            }
            edges {
                node {
                    key
                    name
                    model
                    category {
                        key
                        name
                    }
                    price
                    status
                    boxQuantity
                    createdAt
                    updatedAt
                    options {
                        key
                        price
                        quantity
                        optionAttributes {
                            name
                            value
                        }
                    }
                    images(size: large)
                }
            }
        }
    }
    """
    
    logger.info("ğŸ“¦ allItems GraphQL ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
    logger.info("   (ì „ì²´ ìƒí’ˆ ì¡°íšŒ - ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    result = await collector._make_graphql_request(query, account_name)
    
    if not result or "data" not in result:
        logger.error("âŒ GraphQL ì¿¼ë¦¬ ì‹¤íŒ¨")
        return {"status": "error"}
    
    # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
    all_edges = []
    page_info = result["data"]["allItems"]["pageInfo"]
    all_edges.extend(result["data"]["allItems"]["edges"])
    
    # ì¶”ê°€ í˜ì´ì§€ ì¡°íšŒ (hasNextPageê°€ trueì´ë©´)
    while page_info.get("hasNextPage"):
        logger.info(f"   ë‹¤ìŒ í˜ì´ì§€ ì¡°íšŒ ì¤‘... (ëˆ„ì : {len(all_edges)}ê°œ)")
        
        after_cursor = page_info.get("endCursor")
        paginated_query = query.replace("allItems(first: 1000)", f'allItems(first: 1000, after: "{after_cursor}")')
        
        result = await collector._make_graphql_request(paginated_query, account_name)
        
        if not result or "data" not in result:
            logger.warning("í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨")
            break
        
        page_info = result["data"]["allItems"]["pageInfo"]
        all_edges.extend(result["data"]["allItems"]["edges"])
        
        await asyncio.sleep(0.5)
    
    collection_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"âœ… GraphQL ì‘ë‹µ ìˆ˜ì‹ : {len(all_edges)}ê°œ ìƒí’ˆ")
    logger.info(f"   ì‘ë‹µ ì‹œê°„: {collection_time:.2f}ì´ˆ")
    
    # ìƒí’ˆ ë°ì´í„° ë³€í™˜ ë° ì¤‘ë³µ ì œê±°
    logger.info(f"\nğŸ”„ ìƒí’ˆ ë°ì´í„° ë³€í™˜ ì¤‘...")
    
    products_dict = {}  # supplier_keyë¥¼ í‚¤ë¡œ ì‚¬ìš©í•˜ì—¬ ì¤‘ë³µ ì œê±°
    inactive_count = 0
    duplicate_count = 0
    
    for edge in all_edges:
        node = edge["node"]
        
        # ë¹„í™œì„± ìƒí’ˆ ì œì™¸ (statusê°€ "available"ì´ ì•„ë‹Œ ê²½ìš°)
        # OwnerClan status: available, unavailable, discontinued ë“±
        status = node.get("status", "")
        if status != "available":
            inactive_count += 1
            continue
        
        supplier_key = node["key"]
        
        # ì¤‘ë³µ ê²€ì‚¬ (ê°™ì€ supplier_keyê°€ ì´ë¯¸ ìˆìœ¼ë©´ ìµœì‹  ê²ƒìœ¼ë¡œ ë®ì–´ì“°ê¸°)
        if supplier_key in products_dict:
            duplicate_count += 1
        
        category = node.get("category", {})
        
        product = {
            "supplier_key": supplier_key,
            "name": node.get("name", ""),
            "model": node.get("model", ""),
            "category_name": category.get("name", "") if category else "",
            "category_key": category.get("key", "") if category else "",
            "box_quantity": node.get("boxQuantity", 0),
            "price": node.get("price", 0),
            "status": node.get("status", ""),
            "options": node.get("options", []),
            "images": node.get("images", []),
            "created_at": node.get("createdAt", ""),
            "updated_at": node.get("updatedAt", ""),
            "collected_at": datetime.now().isoformat(),
            "account_name": account_name
        }
        
        products_dict[supplier_key] = product
    
    # dictë¥¼ listë¡œ ë³€í™˜
    all_products = list(products_dict.values())
    
    logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(all_products)}ê°œ (ë¹„í™œì„±: {inactive_count}ê°œ, ì¤‘ë³µ ì œê±°: {duplicate_count}ê°œ)")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ ì €ì¥ (ìµœì í™” - ì¤‘ë³µ ì‚¬ì „ í•„í„°ë§)
    logger.info(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ ì €ì¥ ì¤‘...")
    
    # 1ë‹¨ê³„: ê¸°ì¡´ ìƒí’ˆ ID ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (í˜ì´ì§€ë„¤ì´ì…˜ìœ¼ë¡œ ì „ì²´ ì¡°íšŒ)
    logger.info(f"   ê¸°ì¡´ ìƒí’ˆ ID ì¡°íšŒ ì¤‘...")
    from src.services.supabase_client import supabase_client
    
    try:
        existing_ids = set()
        page_size = 10000
        offset = 0
        
        while True:
            existing_response = (
                supabase_client.get_table("raw_product_data")
                .select("supplier_product_id")
                .eq("supplier_id", supplier_id)
                .range(offset, offset + page_size - 1)
                .execute()
            )
            
            if not existing_response.data:
                break
            
            for item in existing_response.data:
                existing_ids.add(item['supplier_product_id'])
            
            if len(existing_response.data) < page_size:
                break
            
            offset += page_size
            logger.info(f"   ê¸°ì¡´ ìƒí’ˆ ì¡°íšŒ ì¤‘... {len(existing_ids)}ê°œ")
        
        logger.info(f"   ê¸°ì¡´ ìƒí’ˆ: {len(existing_ids)}ê°œ")
    except Exception as e:
        logger.warning(f"   ê¸°ì¡´ ìƒí’ˆ ì¡°íšŒ ì‹¤íŒ¨ (ì „ì²´ upsertë¡œ ì§„í–‰): {e}")
        existing_ids = set()
    
    # 2ë‹¨ê³„: ì‹ ê·œ/ì—…ë°ì´íŠ¸ ìƒí’ˆ ë¶„ë¦¬
    logger.info(f"   ì‹ ê·œ/ì—…ë°ì´íŠ¸ ìƒí’ˆ ë¶„ë¥˜ ì¤‘...")
    new_products = []
    update_products = []
    
    for product in all_products:
        try:
            supplier_product_id = product.get("supplier_key", "")
            
            # JSON ì§ë ¬í™”ë¥¼ í•œ ë²ˆë§Œ ìˆ˜í–‰
            raw_data_json = json.dumps(product, ensure_ascii=False)
            
            # ê°„ë‹¨í•œ í•´ì‹œ ê³„ì‚°
            data_hash = hashlib.md5(f"{supplier_product_id}{product.get('collected_at')}".encode()).hexdigest()
            
            raw_data = {
                "supplier_id": supplier_id,
                "raw_data": raw_data_json,
                "collection_method": "api",
                "collection_source": "batch_allItems_graphql",
                "supplier_product_id": supplier_product_id,
                "is_processed": False,
                "data_hash": data_hash,
                "metadata": json.dumps({
                    "collected_at": product.get("collected_at"),
                    "collection_type": "batch_full_catalog"
                }, ensure_ascii=False)
            }
            
            # ì‹ ê·œ/ì—…ë°ì´íŠ¸ ë¶„ë¥˜
            if supplier_product_id in existing_ids:
                update_products.append(raw_data)
            else:
                new_products.append(raw_data)
                
        except Exception as e:
            logger.warning(f"   ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   ì‹ ê·œ ìƒí’ˆ: {len(new_products)}ê°œ, ì—…ë°ì´íŠ¸ ìƒí’ˆ: {len(update_products)}ê°œ")
    
    # 3ë‹¨ê³„: ì‹ ê·œ ìƒí’ˆë§Œ ëŒ€ëŸ‰ ì‚½ì… (í›¨ì”¬ ë¹ ë¦„)
    saved = 0
    new = 0
    updated = 0
    
    if new_products:
        logger.info(f"\n   ì‹ ê·œ ìƒí’ˆ ì €ì¥ ì¤‘: {len(new_products)}ê°œ")
        batch_size = 5000
        total_batches = (len(new_products) + batch_size - 1) // batch_size
        
        for i in range(0, len(new_products), batch_size):
            chunk = new_products[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            try:
                saved_count = await db.bulk_insert("raw_product_data", chunk)
                new += saved_count
                saved += saved_count
                progress = (batch_num / total_batches) * 100
                logger.info(f"   ì‹ ê·œ ë°°ì¹˜ {batch_num}/{total_batches}: {saved_count}ê°œ (ì§„í–‰ë¥ : {progress:.1f}%)")
            except Exception as e:
                logger.error(f"   ì‹ ê·œ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ì‹œ upsertë¡œ ì¬ì‹œë„ (ì¤‘ë³µ ì²˜ë¦¬)
                try:
                    saved_count = await db.bulk_upsert("raw_product_data", chunk)
                    new += saved_count
                    saved += saved_count
                except Exception as e2:
                    logger.error(f"   upsertë„ ì‹¤íŒ¨: {e2}")
    
    # 4ë‹¨ê³„: ì—…ë°ì´íŠ¸ ìƒí’ˆ ì²˜ë¦¬ (ë³€ê²½ëœ ê²ƒë§Œ)
    if update_products:
        logger.info(f"\n   ì—…ë°ì´íŠ¸ ìƒí’ˆ ì €ì¥ ì¤‘: {len(update_products)}ê°œ")
        batch_size = 5000
        total_batches = (len(update_products) + batch_size - 1) // batch_size
        
        for i in range(0, len(update_products), batch_size):
            chunk = update_products[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            try:
                saved_count = await db.bulk_upsert("raw_product_data", chunk)
                updated += saved_count
                saved += saved_count
                progress = (batch_num / total_batches) * 100
                logger.info(f"   ì—…ë°ì´íŠ¸ ë°°ì¹˜ {batch_num}/{total_batches}: {saved_count}ê°œ (ì§„í–‰ë¥ : {progress:.1f}%)")
            except Exception as e:
                logger.error(f"   ì—…ë°ì´íŠ¸ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"\n{'='*70}")
    logger.info("âœ… ì˜¤ë„ˆí´ëœ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ!")
    logger.info(f"{'='*70}")
    logger.info(f"   API ì‘ë‹µ: {len(all_edges)}ê°œ")
    logger.info(f"   í™œì„± ìƒí’ˆ: {len(all_products)}ê°œ")
    logger.info(f"   ë¹„í™œì„± ìƒí’ˆ: {inactive_count}ê°œ")
    logger.info(f"   ì‹ ê·œ ì €ì¥: {new}ê°œ")
    logger.info(f"   ì—…ë°ì´íŠ¸: {updated}ê°œ")
    logger.info(f"   ì´ ì €ì¥: {saved}ê°œ")
    logger.info(f"   ì´ ì‹œê°„: {total_time/60:.2f}ë¶„")
    logger.info(f"{'='*70}")
    
    result = {
        "status": "success",
        "total_from_api": len(all_edges),
        "active_products": len(all_products),
        "inactive_products": inactive_count,
        "new": new,
        "updated": updated,
        "total_saved": saved,
        "collection_time": collection_time,
        "total_time": total_time
    }
    
    with open('ownerclan_batch_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info("ğŸ’¾ ê²°ê³¼ê°€ ownerclan_batch_result.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    return result


if __name__ == "__main__":
    asyncio.run(collect_ownerclan_full_catalog())

