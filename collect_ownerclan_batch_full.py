#!/usr/bin/env python3
"""
ì˜¤ë„ˆí´ëœ ë°°ì¹˜ íƒ€ì… ì „ì²´ ìˆ˜ì§‘
allItems GraphQL ì¿¼ë¦¬ë¡œ ì „ì²´ ì¹´íƒˆë¡œê·¸ ìˆ˜ì§‘
"""

import asyncio
import json
import hashlib
from datetime import datetime
from loguru import logger

from src.services.ownerclan_data_collector import OwnerClanDataCollector
from src.services.database_service import DatabaseService


async def collect_ownerclan_full_catalog():
    """ì˜¤ë„ˆí´ëœ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘"""
    db = DatabaseService()
    collector = OwnerClanDataCollector()
    
    supplier_id = "e458e4e2-cb03-4fc2-bff1-b05aaffde00f"
    account_name = "test_account"
    
    logger.info("="*70)
    logger.info("ğŸ¢ ì˜¤ë„ˆí´ëœ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘")
    logger.info("   ë°©ì‹: allItems GraphQL ì¿¼ë¦¬ (ì „ì²´ ìƒí’ˆ)")
    logger.info("="*70)
    
    start_time = datetime.now()
    
    # allItems GraphQL ì¿¼ë¦¬ (ì „ì²´ ìƒí’ˆ)
    query = """
    query {
        allItems(first: 1000) {
            pageInfo {
                hasNextPage
                endCursor
            }
            edges {
                node {
                    key
                    name
                    model
                    category {
                        key
                        name
                    }
                    price
                    status
                    boxQuantity
                    createdAt
                    updatedAt
                    options {
                        key
                        price
                        quantity
                        optionAttributes {
                            name
                            value
                        }
                    }
                    images(size: large)
                }
            }
        }
    }
    """
    
    logger.info("ğŸ“¦ allItems GraphQL ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘...")
    logger.info("   (ì „ì²´ ìƒí’ˆ ì¡°íšŒ - ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    result = await collector._make_graphql_request(query, account_name)
    
    if not result or "data" not in result:
        logger.error("âŒ GraphQL ì¿¼ë¦¬ ì‹¤íŒ¨")
        return {"status": "error"}
    
    # í˜ì´ì§€ë„¤ì´ì…˜ ì²˜ë¦¬
    all_edges = []
    page_info = result["data"]["allItems"]["pageInfo"]
    all_edges.extend(result["data"]["allItems"]["edges"])
    
    # ì¶”ê°€ í˜ì´ì§€ ì¡°íšŒ (hasNextPageê°€ trueì´ë©´)
    while page_info.get("hasNextPage"):
        logger.info(f"   ë‹¤ìŒ í˜ì´ì§€ ì¡°íšŒ ì¤‘... (ëˆ„ì : {len(all_edges)}ê°œ)")
        
        after_cursor = page_info.get("endCursor")
        paginated_query = query.replace("allItems(first: 1000)", f'allItems(first: 1000, after: "{after_cursor}")')
        
        result = await collector._make_graphql_request(paginated_query, account_name)
        
        if not result or "data" not in result:
            logger.warning("í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨")
            break
        
        page_info = result["data"]["allItems"]["pageInfo"]
        all_edges.extend(result["data"]["allItems"]["edges"])
        
        await asyncio.sleep(0.5)
    
    collection_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"âœ… GraphQL ì‘ë‹µ ìˆ˜ì‹ : {len(all_edges)}ê°œ ìƒí’ˆ")
    logger.info(f"   ì‘ë‹µ ì‹œê°„: {collection_time:.2f}ì´ˆ")
    
    # ìƒí’ˆ ë°ì´í„° ë³€í™˜
    logger.info(f"\nğŸ”„ ìƒí’ˆ ë°ì´í„° ë³€í™˜ ì¤‘...")
    
    all_products = []
    inactive_count = 0
    
    for edge in all_edges:
        node = edge["node"]
        
        # ë¹„í™œì„± ìƒí’ˆ ì œì™¸ (statusê°€ "available"ì´ ì•„ë‹Œ ê²½ìš°)
        # OwnerClan status: available, unavailable, discontinued ë“±
        status = node.get("status", "")
        if status != "available":
            inactive_count += 1
            continue
        
        category = node.get("category", {})
        
        product = {
            "supplier_key": node["key"],
            "name": node.get("name", ""),
            "model": node.get("model", ""),
            "category_name": category.get("name", "") if category else "",
            "category_key": category.get("key", "") if category else "",
            "box_quantity": node.get("boxQuantity", 0),
            "price": node.get("price", 0),
            "status": node.get("status", ""),
            "options": node.get("options", []),
            "images": node.get("images", []),
            "created_at": node.get("createdAt", ""),
            "updated_at": node.get("updatedAt", ""),
            "collected_at": datetime.now().isoformat(),
            "account_name": account_name
        }
        
        all_products.append(product)
    
    logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(all_products)}ê°œ (ë¹„í™œì„± ì œì™¸: {inactive_count}ê°œ)")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
    logger.info(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì¤‘...")
    
    saved = 0
    new = 0
    updated = 0
    
    for idx, product in enumerate(all_products, 1):
        try:
            supplier_product_id = product.get("supplier_key", "")
            
            data_str = json.dumps(product, sort_keys=True, ensure_ascii=False)
            data_hash = hashlib.md5(data_str.encode('utf-8')).hexdigest()
            
            raw_data = {
                "supplier_id": supplier_id,
                "raw_data": json.dumps(product, ensure_ascii=False),
                "collection_method": "api",
                "collection_source": "batch_allItems_graphql",
                "supplier_product_id": supplier_product_id,
                "is_processed": False,
                "data_hash": data_hash,
                "metadata": json.dumps({
                    "collected_at": product.get("collected_at"),
                    "collection_type": "batch_full_catalog"
                }, ensure_ascii=False)
            }
            
            existing = await db.select_data(
                "raw_product_data",
                {"supplier_id": supplier_id, "supplier_product_id": supplier_product_id}
            )
            
            if existing:
                await db.update_data("raw_product_data", raw_data, {"id": existing[0]["id"]})
                updated += 1
            else:
                await db.insert_data("raw_product_data", raw_data)
                new += 1
            
            saved += 1
            
            if idx % 500 == 0:
                logger.info(f"   ì§„í–‰: {idx}/{len(all_products)}ê°œ (ì‹ ê·œ: {new}, ì—…ë°ì´íŠ¸: {updated})")
                
        except Exception as e:
            logger.warning(f"   ì €ì¥ ì‹¤íŒ¨: {e}")
            continue
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"\n{'='*70}")
    logger.info("âœ… ì˜¤ë„ˆí´ëœ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ!")
    logger.info(f"{'='*70}")
    logger.info(f"   API ì‘ë‹µ: {len(all_edges)}ê°œ")
    logger.info(f"   í™œì„± ìƒí’ˆ: {len(all_products)}ê°œ")
    logger.info(f"   ë¹„í™œì„± ìƒí’ˆ: {inactive_count}ê°œ")
    logger.info(f"   ì‹ ê·œ ì €ì¥: {new}ê°œ")
    logger.info(f"   ì—…ë°ì´íŠ¸: {updated}ê°œ")
    logger.info(f"   ì´ ì €ì¥: {saved}ê°œ")
    logger.info(f"   ì´ ì‹œê°„: {total_time/60:.2f}ë¶„")
    logger.info(f"{'='*70}")
    
    result = {
        "status": "success",
        "total_from_api": len(all_edges),
        "active_products": len(all_products),
        "inactive_products": inactive_count,
        "new": new,
        "updated": updated,
        "total_saved": saved,
        "collection_time": collection_time,
        "total_time": total_time
    }
    
    with open('ownerclan_batch_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info("ğŸ’¾ ê²°ê³¼ê°€ ownerclan_batch_result.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    return result


if __name__ == "__main__":
    asyncio.run(collect_ownerclan_full_catalog())

