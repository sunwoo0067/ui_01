#!/usr/bin/env python3
"""
ë„ë§¤ê¾¹ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘ (ìµœì í™”)
"""

import asyncio
import json
import hashlib
from datetime import datetime
from loguru import logger

from src.services.domaemae_data_collector import DomaemaeDataCollector
from src.services.database_service import DatabaseService


async def collect_domaemae_full_catalog():
    """ë„ë§¤ê¾¹ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘"""
    db = DatabaseService()
    collector = DomaemaeDataCollector(db)
    
    # ë„ë§¤ê¾¹ supplier_id (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í™•ì¸ í•„ìš”)
    suppliers = await db.select_data("suppliers", {"code": "domaemae"})
    if not suppliers:
        logger.error("ë„ë§¤ê¾¹ ê³µê¸‰ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {"status": "error"}
    
    supplier_id = suppliers[0]["id"]
    account_name = "test_account"
    
    logger.info("="*70)
    logger.info("ğŸ¢ ë„ë§¤ê¾¹ ì „ì²´ ì¹´íƒˆë¡œê·¸ ë°°ì¹˜ ìˆ˜ì§‘")
    logger.info("   ë°©ì‹: ë„ë§¤ê¾¹ + ë„ë§¤ë§¤ ëª¨ë“  ìƒí’ˆ")
    logger.info("="*70)
    
    start_time = datetime.now()
    
    # 1. ë„ë§¤ê¾¹(dome) ìƒí’ˆ ìˆ˜ì§‘
    logger.info("\nğŸ“¦ ë„ë§¤ê¾¹(dome) ìƒí’ˆ ìˆ˜ì§‘ ì¤‘...")
    dome_products = await collector.collect_products_batch(
        account_name=account_name,
        batch_size=500,
        max_pages=None,  # ì „ì²´ ìˆ˜ì§‘
        market="dome"
    )
    
    logger.info(f"âœ… ë„ë§¤ê¾¹ ìˆ˜ì§‘ ì™„ë£Œ: {len(dome_products)}ê°œ")
    
    # 2. ë„ë§¤ë§¤(supply) ìƒí’ˆ ìˆ˜ì§‘
    logger.info("\nğŸ“¦ ë„ë§¤ë§¤(supply) ìƒí’ˆ ìˆ˜ì§‘ ì¤‘...")
    supply_products = await collector.collect_products_batch(
        account_name=account_name,
        batch_size=500,
        max_pages=None,  # ì „ì²´ ìˆ˜ì§‘
        market="supply"
    )
    
    logger.info(f"âœ… ë„ë§¤ë§¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(supply_products)}ê°œ")
    
    # 3. í•©ì¹˜ê¸°
    all_products = dome_products + supply_products
    collection_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"\nâœ… ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_products)}ê°œ")
    logger.info(f"   ë„ë§¤ê¾¹: {len(dome_products)}ê°œ")
    logger.info(f"   ë„ë§¤ë§¤: {len(supply_products)}ê°œ")
    logger.info(f"   ìˆ˜ì§‘ ì‹œê°„: {collection_time:.2f}ì´ˆ")
    
    # ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ ì €ì¥ (ìµœì í™”)
    logger.info(f"\nğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë°°ì¹˜ ì €ì¥ ì¤‘...")
    
    # 1ë‹¨ê³„: ê¸°ì¡´ ìƒí’ˆ ID ì¡°íšŒ
    logger.info("   ê¸°ì¡´ ìƒí’ˆ ID ì¡°íšŒ ì¤‘...")
    from src.services.supabase_client import supabase_client
    
    try:
        existing_ids = set()
        page_size = 10000
        offset = 0
        
        while True:
            existing_response = (
                supabase_client.get_table("raw_product_data")
                .select("supplier_product_id")
                .eq("supplier_id", supplier_id)
                .range(offset, offset + page_size - 1)
                .execute()
            )
            
            if not existing_response.data:
                break
            
            for item in existing_response.data:
                existing_ids.add(item['supplier_product_id'])
            
            if len(existing_response.data) < page_size:
                break
            
            offset += page_size
        
        logger.info(f"   ê¸°ì¡´ ìƒí’ˆ: {len(existing_ids)}ê°œ")
    except Exception as e:
        logger.warning(f"   ê¸°ì¡´ ìƒí’ˆ ì¡°íšŒ ì‹¤íŒ¨ (ì „ì²´ upsertë¡œ ì§„í–‰): {e}")
        existing_ids = set()
    
    # 2ë‹¨ê³„: ì‹ ê·œ/ì—…ë°ì´íŠ¸ ìƒí’ˆ ë¶„ë¦¬
    logger.info("   ì‹ ê·œ/ì—…ë°ì´íŠ¸ ìƒí’ˆ ë¶„ë¥˜ ì¤‘...")
    new_products = []
    update_products = []
    
    for product in all_products:
        try:
            supplier_product_id = str(product.get("item_id", ""))
            
            # JSON ì§ë ¬í™”ë¥¼ í•œ ë²ˆë§Œ ìˆ˜í–‰
            raw_data_json = json.dumps(product, ensure_ascii=False)
            data_hash = hashlib.md5(f"{supplier_product_id}{datetime.now().isoformat()}".encode()).hexdigest()
            
            raw_data = {
                "supplier_id": supplier_id,
                "raw_data": raw_data_json,
                "collection_method": "api",
                "collection_source": "http://api.domaemae.co.kr/openapi",
                "supplier_product_id": supplier_product_id,
                "is_processed": False,
                "data_hash": data_hash,
                "metadata": json.dumps({
                    "collected_at": product.get("collected_at", datetime.now().isoformat()),
                    "account_name": account_name,
                    "market": product.get("market", "")
                }, ensure_ascii=False)
            }
            
            # ì‹ ê·œ/ì—…ë°ì´íŠ¸ ë¶„ë¥˜
            if supplier_product_id in existing_ids:
                update_products.append(raw_data)
            else:
                new_products.append(raw_data)
                
        except Exception as e:
            logger.warning(f"   ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"   ì‹ ê·œ: {len(new_products)}ê°œ, ì—…ë°ì´íŠ¸: {len(update_products)}ê°œ")
    
    # 3ë‹¨ê³„: ë°°ì¹˜ ì €ì¥
    saved = 0
    new = 0
    updated = 0
    batch_size = 5000
    
    # ì‹ ê·œ ìƒí’ˆ bulk insert
    if new_products:
        logger.info(f"\n   ì‹ ê·œ ìƒí’ˆ ì €ì¥ ì¤‘: {len(new_products)}ê°œ")
        total_batches = (len(new_products) + batch_size - 1) // batch_size
        
        for i in range(0, len(new_products), batch_size):
            chunk = new_products[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            try:
                count = await db.bulk_insert("raw_product_data", chunk)
                new += count
                saved += count
                progress = (batch_num / total_batches) * 100
                logger.info(f"   ì‹ ê·œ ë°°ì¹˜ {batch_num}/{total_batches}: {count}ê°œ ({progress:.1f}%)")
            except Exception as e:
                logger.error(f"   ì‹ ê·œ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ì‹œ upsertë¡œ ì¬ì‹œë„
                try:
                    count = await db.bulk_upsert("raw_product_data", chunk)
                    new += count
                    saved += count
                except:
                    pass
    
    # ì—…ë°ì´íŠ¸ ìƒí’ˆ bulk upsert
    if update_products:
        logger.info(f"\n   ì—…ë°ì´íŠ¸ ìƒí’ˆ ì €ì¥ ì¤‘: {len(update_products)}ê°œ")
        total_batches = (len(update_products) + batch_size - 1) // batch_size
        
        for i in range(0, len(update_products), batch_size):
            chunk = update_products[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            try:
                count = await db.bulk_upsert("raw_product_data", chunk)
                updated += count
                saved += count
                progress = (batch_num / total_batches) * 100
                logger.info(f"   ì—…ë°ì´íŠ¸ ë°°ì¹˜ {batch_num}/{total_batches}: {count}ê°œ ({progress:.1f}%)")
            except Exception as e:
                logger.error(f"   ì—…ë°ì´íŠ¸ ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"\n{'='*70}")
    logger.info("âœ… ë„ë§¤ê¾¹ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ!")
    logger.info(f"{'='*70}")
    logger.info(f"   ìˆ˜ì§‘ ìƒí’ˆ: {len(all_products)}ê°œ (ë„ë§¤ê¾¹: {len(dome_products)}, ë„ë§¤ë§¤: {len(supply_products)})")
    logger.info(f"   ì‹ ê·œ ì €ì¥: {new}ê°œ")
    logger.info(f"   ì—…ë°ì´íŠ¸: {updated}ê°œ")
    logger.info(f"   ì´ ì €ì¥: {saved}ê°œ")
    logger.info(f"   ì´ ì‹œê°„: {total_time/60:.2f}ë¶„")
    logger.info(f"{'='*70}")
    
    result = {
        "status": "success",
        "total_collected": len(all_products),
        "dome_products": len(dome_products),
        "supply_products": len(supply_products),
        "new": new,
        "updated": updated,
        "total_saved": saved,
        "collection_time": collection_time,
        "total_time": total_time
    }
    
    with open('domaemae_batch_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info("ğŸ’¾ ê²°ê³¼ê°€ domaemae_batch_result.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    return result


if __name__ == "__main__":
    asyncio.run(collect_domaemae_full_catalog())

