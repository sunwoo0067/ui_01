#!/usr/bin/env python3
"""
ëŒ€ëŸ‰ ë°ì´í„° ì •ê·œí™” ì²˜ë¦¬ (ìµœì í™”)
65,000ê°œ ì´ìƒì˜ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì •ê·œí™”
"""

import asyncio
import json
from datetime import datetime
from uuid import UUID
from loguru import logger

from src.services.database_service import DatabaseService
from src.services.connectors import ConnectorFactory
from src.services.supabase_client import supabase_client


async def bulk_normalize_products(supplier_id: str, batch_size: int = 1000, max_count: int = None):
    """ëŒ€ëŸ‰ ìƒí’ˆ ì •ê·œí™” ì²˜ë¦¬"""
    
    db = DatabaseService()
    
    logger.info("="*70)
    logger.info("ğŸ”„ ëŒ€ëŸ‰ ìƒí’ˆ ì •ê·œí™” ì‹œì‘")
    logger.info(f"   ê³µê¸‰ì‚¬ ID: {supplier_id}")
    logger.info(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
    logger.info(f"   ìµœëŒ€ ì²˜ë¦¬: {max_count if max_count else 'ì „ì²´'}")
    logger.info("="*70)
    
    start_time = datetime.now()
    
    # 1ë‹¨ê³„: ê³µê¸‰ì‚¬ ì •ë³´ ì¡°íšŒ
    logger.info("\n1ï¸âƒ£ ê³µê¸‰ì‚¬ ì •ë³´ ì¡°íšŒ ì¤‘...")
    suppliers = await db.select_data("suppliers", {"id": supplier_id})
    
    if not suppliers:
        logger.error(f"ê³µê¸‰ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {supplier_id}")
        return {"status": "error", "message": "Supplier not found"}
    
    supplier = suppliers[0]
    logger.info(f"   ê³µê¸‰ì‚¬: {supplier['name']} ({supplier['code']})")
    
    # 2ë‹¨ê³„: ê³„ì • ì •ë³´ ì¡°íšŒ
    logger.info("\n2ï¸âƒ£ ê³„ì • ì •ë³´ ì¡°íšŒ ì¤‘...")
    accounts = await db.select_data("supplier_accounts", {"supplier_id": supplier_id, "is_active": True})
    
    if not accounts:
        logger.error(f"í™œì„± ê³„ì •ì´ ì—†ìŠµë‹ˆë‹¤: {supplier_id}")
        return {"status": "error", "message": "No active accounts"}
    
    account = accounts[0]
    logger.info(f"   ê³„ì •: {account['account_name']}")
    
    # 3ë‹¨ê³„: ì»¤ë„¥í„° ìƒì„±
    logger.info("\n3ï¸âƒ£ ì»¤ë„¥í„° ìƒì„± ì¤‘...")
    
    credentials = json.loads(account['account_credentials']) if isinstance(account['account_credentials'], str) else account['account_credentials']
    
    # api_config ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    api_config = supplier.get('api_config', {})
    if isinstance(api_config, str):
        api_config = json.loads(api_config)
    if not api_config:
        api_config = {}
    
    from src.services.connectors import CollectionMethod
    
    # supplier type ê°€ì ¸ì˜¤ê¸°
    supplier_type_str = supplier.get('type', 'api')
    supplier_type = CollectionMethod(supplier_type_str)
    
    connector = ConnectorFactory.create(
        supplier_code=supplier['code'],
        supplier_id=UUID(supplier_id),
        supplier_type=supplier_type,
        credentials=credentials,
        config=api_config
    )
    
    logger.info(f"   ì»¤ë„¥í„°: {connector.__class__.__name__}")
    
    # 4ë‹¨ê³„: ë¯¸ì²˜ë¦¬ ë°ì´í„° ì¡°íšŒ
    logger.info("\n4ï¸âƒ£ ë¯¸ì²˜ë¦¬ ë°ì´í„° ì¡°íšŒ ì¤‘...")
    
    offset = 0
    total_raw_count = 0
    
    # ì „ì²´ ê°œìˆ˜ í™•ì¸
    count_response = (
        supabase_client.get_table("raw_product_data")
        .select("id", count="exact")
        .eq("supplier_id", supplier_id)
        .eq("is_processed", False)
        .execute()
    )
    
    total_raw_count = count_response.count
    logger.info(f"   ë¯¸ì²˜ë¦¬ ë°ì´í„°: {total_raw_count:,}ê°œ")
    
    if max_count:
        total_raw_count = min(total_raw_count, max_count)
        logger.info(f"   ì²˜ë¦¬ ì œí•œ: {total_raw_count:,}ê°œ")
    
    # 5ë‹¨ê³„: ë°°ì¹˜ ì²˜ë¦¬
    logger.info(f"\n5ï¸âƒ£ ë°°ì¹˜ ì •ê·œí™” ì‹œì‘ (ë°°ì¹˜ í¬ê¸°: {batch_size})...")
    
    success_count = 0
    failed_count = 0
    batch_num = 0
    total_batches = (total_raw_count + batch_size - 1) // batch_size
    
    while offset < total_raw_count:
        batch_num += 1
        
        # ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
        logger.info(f"\n   ë°°ì¹˜ {batch_num}/{total_batches} ì¡°íšŒ ì¤‘... (offset: {offset})")
        
        raw_data_batch = (
            supabase_client.get_table("raw_product_data")
            .select("*")
            .eq("supplier_id", supplier_id)
            .eq("is_processed", False)
            .range(offset, offset + batch_size - 1)
            .execute()
        )
        
        if not raw_data_batch.data:
            logger.info("   ë” ì´ìƒ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            break
        
        batch_items = raw_data_batch.data
        logger.info(f"   ë°°ì¹˜ {batch_num}: {len(batch_items)}ê°œ ì •ê·œí™” ì¤‘...")
        
        # ì •ê·œí™”ëœ ìƒí’ˆ ë¦¬ìŠ¤íŠ¸
        normalized_batch = []
        processed_ids = []
        
        for idx, raw_item in enumerate(batch_items, 1):
            try:
                # raw_data íŒŒì‹±
                raw_data = raw_item['raw_data']
                if isinstance(raw_data, str):
                    raw_data = json.loads(raw_data)
                
                # ì»¤ë„¥í„°ë¡œ ë³€í™˜
                normalized_data = await connector.transform_product(raw_data)
                
                # ì •ê·œí™”ëœ ìƒí’ˆ ë°ì´í„°
                normalized_product = {
                    "raw_data_id": raw_item['id'],
                    "supplier_id": supplier_id,
                    "supplier_product_id": normalized_data.get('supplier_product_id', ''),
                    "title": normalized_data.get('title', ''),
                    "description": normalized_data.get('description', ''),
                    "price": float(normalized_data.get('price', 0)),
                    "cost_price": float(normalized_data.get('cost_price', 0)),
                    "currency": normalized_data.get('currency', 'KRW'),
                    "category": normalized_data.get('category', ''),
                    "brand": normalized_data.get('brand', ''),
                    "stock_quantity": int(normalized_data.get('stock_quantity', 0)),
                    "status": normalized_data.get('status', 'active'),
                    "images": json.dumps(normalized_data.get('images', []), ensure_ascii=False),
                    "attributes": json.dumps(normalized_data.get('attributes', {}), ensure_ascii=False)
                }
                
                normalized_batch.append(normalized_product)
                processed_ids.append(raw_item['id'])
                
                if idx % 100 == 0:
                    logger.info(f"      ì§„í–‰: {idx}/{len(batch_items)}ê°œ")
                
            except Exception as e:
                logger.warning(f"      ì •ê·œí™” ì‹¤íŒ¨: {raw_item.get('id')}, {e}")
                failed_count += 1
                continue
        
        # 6ë‹¨ê³„: ì •ê·œí™” ë°ì´í„° bulk insert
        if normalized_batch:
            logger.info(f"   ë°°ì¹˜ {batch_num}: {len(normalized_batch)}ê°œ ì €ì¥ ì¤‘...")
            
            try:
                # bulk insertë¡œ ì €ì¥
                saved_count = await db.bulk_insert("normalized_products", normalized_batch)
                success_count += saved_count
                logger.info(f"   ë°°ì¹˜ {batch_num} ì €ì¥ ì™„ë£Œ: {saved_count}ê°œ")
                
            except Exception as e:
                logger.error(f"   ë°°ì¹˜ {batch_num} ì €ì¥ ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨ì‹œ bulk upsertë¡œ ì¬ì‹œë„
                try:
                    saved_count = await db.bulk_upsert("normalized_products", normalized_batch)
                    success_count += saved_count
                    logger.info(f"   ë°°ì¹˜ {batch_num} upsert ì™„ë£Œ: {saved_count}ê°œ")
                except Exception as e2:
                    logger.error(f"   upsertë„ ì‹¤íŒ¨: {e2}")
                    failed_count += len(normalized_batch)
        
        # 7ë‹¨ê³„: ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ (ì‘ì€ ë°°ì¹˜ë¡œ)
        if processed_ids:
            logger.info(f"   ë°°ì¹˜ {batch_num}: {len(processed_ids)}ê°œ ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ ì¤‘...")
            
            try:
                # 100ê°œì”© ë‚˜ëˆ ì„œ ì—…ë°ì´íŠ¸ (414 ì—ëŸ¬ ë°©ì§€)
                update_batch_size = 100
                for i in range(0, len(processed_ids), update_batch_size):
                    id_chunk = processed_ids[i:i + update_batch_size]
                    
                    update_query = (
                        supabase_client.get_table("raw_product_data")
                        .update({"is_processed": True, "processed_at": datetime.now().isoformat()})
                        .in_("id", id_chunk)
                        .execute()
                    )
                
                logger.info(f"   ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"   ì²˜ë¦¬ ì™„ë£Œ í‘œì‹œ ì‹¤íŒ¨: {e}")
        
        # ì§„í–‰ë¥  ê³„ì‚°
        progress = ((offset + len(batch_items)) / total_raw_count) * 100
        logger.info(f"   ë°°ì¹˜ {batch_num} ì™„ë£Œ: ì„±ê³µ {len(normalized_batch)}ê°œ (ëˆ„ì : {success_count}/{total_raw_count}, ì§„í–‰ë¥ : {progress:.1f}%)")
        
        offset += batch_size
        
        # API í˜¸ì¶œ ê°„ê²©
        await asyncio.sleep(0.1)
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    logger.info(f"\n{'='*70}")
    logger.info("âœ… ëŒ€ëŸ‰ ì •ê·œí™” ì™„ë£Œ!")
    logger.info(f"{'='*70}")
    logger.info(f"   ì²˜ë¦¬ ë°ì´í„°: {total_raw_count:,}ê°œ")
    logger.info(f"   ì„±ê³µ: {success_count:,}ê°œ")
    logger.info(f"   ì‹¤íŒ¨: {failed_count:,}ê°œ")
    logger.info(f"   ì„±ê³µë¥ : {(success_count/total_raw_count*100):.1f}%" if total_raw_count > 0 else "   ì„±ê³µë¥ : N/A")
    logger.info(f"   ì´ ì‹œê°„: {total_time/60:.2f}ë¶„")
    logger.info(f"   ì²˜ë¦¬ ì†ë„: {success_count/total_time:.1f}ê°œ/ì´ˆ" if total_time > 0 else "   ì²˜ë¦¬ ì†ë„: N/A")
    logger.info(f"{'='*70}")
    
    result = {
        "status": "success",
        "total": total_raw_count,
        "success": success_count,
        "failed": failed_count,
        "success_rate": (success_count/total_raw_count*100) if total_raw_count > 0 else 0,
        "total_time": total_time,
        "processing_speed": success_count/total_time if total_time > 0 else 0
    }
    
    with open('bulk_normalization_result.json', 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info("ğŸ’¾ ê²°ê³¼ê°€ bulk_normalization_result.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")
    
    return result


if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹ì¤„ ì¸ìë¡œ ê³µê¸‰ì‚¬ì™€ ìµœëŒ€ ê°œìˆ˜ ì„¤ì • ê°€ëŠ¥
    # ì‚¬ìš©ë²•: python process_bulk_normalization.py [supplier_code] [max_count]
    
    supplier_codes = {
        'ownerclan': 'e458e4e2-cb03-4fc2-bff1-b05aaffde00f',
        'zentrade': '959ddf49-c25f-4ebb-a292-bc4e0f1cd28a',
        'domaemae': 'baa2ccd3-a328-4387-b307-6ae89aea331b'
    }
    
    if len(sys.argv) > 1 and sys.argv[1] in supplier_codes:
        supplier_code = sys.argv[1]
        supplier_id = supplier_codes[supplier_code]
        max_count = int(sys.argv[2]) if len(sys.argv) > 2 else None
    else:
        # ê¸°ë³¸ê°’: ì˜¤ë„ˆí´ëœ
        supplier_id = supplier_codes['ownerclan']
        max_count = int(sys.argv[1]) if len(sys.argv) > 1 else None
    
    # ë°ì´í„° ì •ê·œí™”
    result = asyncio.run(bulk_normalize_products(
        supplier_id=supplier_id,
        batch_size=1000,
        max_count=max_count
    ))

